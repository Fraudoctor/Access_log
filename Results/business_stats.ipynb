{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BigQuery settings\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import dbapi;\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the time matching CSV and remove MRNs for security reasons (ignore if already exists)\n",
    "import pandas as pd\n",
    "data_frame = pd.read_csv('jon_mapping.csv')\n",
    "data_frame = data_frame.drop('MRN', axis = 1) \n",
    "data_frame.to_csv('tmp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random mapping (CSV file) to test\n",
    "\n",
    "import pandas as pd\n",
    "data_frame = pd.read_csv('tmp.csv')\n",
    "print(list(data_frame.columns)) # print the column names\n",
    "\n",
    "num_row = data_frame.shape[0]\n",
    "print(num_row) # print num_row\n",
    "\n",
    "data_frame = data_frame.drop('JITTER', axis = 1) # remove a column\n",
    "\n",
    "data_frame['JITTER_test']= np.random.randint(10, size=num_row) # add a column \n",
    "\n",
    "print(data_frame[0:10]) # print the first 10 rows\n",
    "\n",
    "data_frame.to_csv('tmp_rnd_shift.csv', index = False) # save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of ADT of cohort patients (ADT_cohort_jit): \n",
    "###.  *** time_out is either the actual time_out or TPA_admin_time, whichever is earlier\n",
    "\n",
    "query =  \"\"\"\n",
    "drop table if exists noshad_test.ADT_cohort_jit;\n",
    "create table noshad_test.ADT_cohort_jit as\n",
    "(\n",
    "SELECT ADT.jc_uid, ADT.pat_enc_csn_id_coded, ADT.department_id, CH.tpaAdminTime,\n",
    "    min(ADT.event_time_jittered) AS time_in, max(ADT.event_time_jittered) AS time_out\n",
    "FROM `starr_datalake2018.adt` AS ADT\n",
    "INNER JOIN `noshad_test.cohort_AL_user_role` AS CH\n",
    "  USING (pat_enc_csn_id_coded)\n",
    "GROUP BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, ADT.department_id, CH.tpaAdminTime\n",
    "ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(query);\n",
    "\n",
    "#results = cursor.fetchall();\n",
    "#print(results[:2])\n",
    "#results_np = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporary tmp file generated\n",
      "feature extraced\n",
      "temporary tmp file deleted\n"
     ]
    }
   ],
   "source": [
    "## main part to generate the number of patients per each cohort_patient\n",
    "\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();\n",
    "\n",
    "# Upload time_mapping tmp.CSV \n",
    "\n",
    "schemafield_col1 = bigquery.schema.SchemaField(\"ANON_ID\",\"STRING\") #Define your schema\n",
    "schemafield_col2 = bigquery.schema.SchemaField(\"JITTER\",\"INTEGER\")\n",
    "\n",
    "filename = 'tmp.csv'\n",
    "table_id = 'tmp' # the name of the chart to create\n",
    "\n",
    "dataset_ref = client.dataset('noshad_test')\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "\n",
    "with open(filename, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print('temporary tmp file generated')\n",
    "\n",
    "# Main part to generate num_pat per each patient\n",
    "query= \"\"\"\n",
    "\n",
    "CREATE OR REPLACE TABLE noshad_test.num_pat AS(\n",
    "WITH\n",
    "\n",
    "    -- Generate ADT_cohort with actual times\n",
    "  ADT_real_date AS\n",
    "  (SELECT ADT.* except(time_in,time_out), DATETIME_SUB(ADT.time_in, INTERVAL TMP.JITTER DAY) as time_in,  \n",
    "  DATETIME_SUB(ADT.time_out, INTERVAL TMP.JITTER DAY) as time_out\n",
    "  \n",
    "  FROM `noshad_test.ADT_cohort_jit` as ADT,\n",
    "  `noshad_test.tmp` as TMP\n",
    "  \n",
    "  WHERE ADT.jc_uid=TMP.ANON_ID\n",
    "  \n",
    "  ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    "  ),\n",
    "  \n",
    "    -- Generate AL with actual times\n",
    "  AL_real_date AS\n",
    "  (SELECT AL.*, DATETIME_SUB(AL.access_time_jittered, INTERVAL TMP.JITTER DAY) as access_time_real\n",
    "  FROM `shc_access_log.shc_access_log_de` as AL,\n",
    "  `noshad_test.tmp` as TMP\n",
    "  WHERE AL.rit_uid=TMP.ANON_ID\n",
    "  ORDER BY AL.rit_uid\n",
    "  ),\n",
    "  \n",
    "  --- Generate NUM of PAT PER DEP with times\n",
    "  NUM_PAT_PER_DEP AS \n",
    "  (SELECT ADT_real_date.*, count(*) as num_tranx , \n",
    "    count(*)/ DATETIME_DIFF(ADT_real_date.time_out, ADT_real_date.time_in, MINUTE) as num_tranx_rate \n",
    "  FROM ADT_real_date, AL_real_date\n",
    "  WHERE ADT_real_date.jc_uid=AL_real_date.rit_uid \n",
    "    AND ADT_real_date.time_in < AL_real_date.access_time_real \n",
    "    AND AL_real_date.access_time_real < ADT_real_date.time_out\n",
    "  GROUP BY ADT_real_date.jc_uid, ADT_real_date.pat_enc_csn_id_coded, \n",
    "    ADT_real_date.department_id, ADT_real_date.tpaAdminTime, \n",
    "    ADT_real_date.time_in, ADT_real_date.time_out\n",
    "  ORDER BY ADT_real_date.department_id)\n",
    "\n",
    "  -- Main script\n",
    "SELECT jc_uid, pat_enc_csn_id_coded, max(num_tranx_rate) as max_norm_num_tranx\n",
    "FROM NUM_PAT_PER_DEP\n",
    "WHERE time_in < tpaAdminTime\n",
    "GROUP BY jc_uid, pat_enc_csn_id_coded\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query);\n",
    "\n",
    "print('feature extraced')\n",
    "\n",
    "## Final step: delete the temporary time mapping\n",
    "query = \"DROP TABLE noshad_test.tmp\"\n",
    "cursor.execute(query);\n",
    "\n",
    "print('temporary tmp file deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Patient Dictionary\n",
    "# we have a dictionary of patients (jc_uid). Each patient has dictionary of different statistics: \n",
    "#          time2tpa, number of unique users, etc\n",
    "\n",
    "\n",
    "Unique_ids, Indx = np.unique(AL[:,1], return_index = True) # unique patiant ids\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Create the patient dictionary and extract TPA times for each patient and put it in a dictionary\n",
    "\n",
    "Time2tpa = np.zeros(Indx.shape)\n",
    "Pat_dic = {}\n",
    "\n",
    "for id in Indx:\n",
    "    t1 = AL[id,5]\n",
    "    t2 = AL[id,9]\n",
    "    #print(t1,t2)\n",
    "    date_time_t1 = datetime.datetime.strptime(t1, '%Y-%m-%d %H:%M:%S')\n",
    "    date_time_t2 = datetime.datetime.strptime(t2, '%Y-%m-%d %H:%M:%S')\n",
    "    delta_t = date_time_t2 - date_time_t1\n",
    "    time_int = int(delta_t.total_seconds() / 60)\n",
    "    \n",
    "    if time_int < 60*10:\n",
    "        Pat_dic[AL[id,1]] = {} #each patient has a dictionary\n",
    "        Pat_dic[AL[id,1]]['t2tpa'] = time_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from `noshad_test.num_pat`\"; # Example dataset table\n",
    "cursor.execute(query);\n",
    "results = cursor.fetchall();\n",
    "Cohort_AL_np = np.array(results)\n",
    "\n",
    "# return in numpy array\n",
    "print(Cohort_AL_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
