{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BigQuery settings\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import dbapi;\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore if local tmp.csv already exists\n",
    "# read the time matching CSV and remove MRNs for security reasons (ignore if local tmp.csv already exists)\n",
    "import pandas as pd\n",
    "data_frame = pd.read_csv('jon_mapping.csv')\n",
    "data_frame = data_frame.drop('MRN', axis = 1) \n",
    "data_frame.to_csv('tmp.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for testing\n",
    "# Generate a random mapping (CSV file) to test (only for testing)\n",
    "\n",
    "import pandas as pd\n",
    "data_frame = pd.read_csv('tmp.csv')\n",
    "print(list(data_frame.columns)) # print the column names\n",
    "\n",
    "num_row = data_frame.shape[0]\n",
    "print(num_row) # print num_row\n",
    "\n",
    "data_frame = data_frame.drop('JITTER', axis = 1) # remove a column\n",
    "\n",
    "data_frame['JITTER_test']= np.random.randint(10, size=num_row) # add a column \n",
    "\n",
    "print(data_frame[0:10]) # print the first 10 rows\n",
    "\n",
    "data_frame.to_csv('tmp_rnd_shift.csv', index = False) # save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if table ADT_cohort_jit already exists on GCP\n",
    "# Create a table of ADT of cohort patients (with jittered times) (ADT_cohort_jit): \n",
    "###.  *** time_out is either the actual time_out or TPA_admin_time, whichever is earlier\n",
    "\n",
    "query =  \"\"\"\n",
    "drop table if exists noshad.ADT_cohort_jit;\n",
    "create table noshad.ADT_cohort_jit as\n",
    "(\n",
    "SELECT ADT.jc_uid, ADT.pat_enc_csn_id_coded, ADT.department_id, CH.tpaAdminTime,\n",
    "    min(ADT.event_time_jittered) AS time_in, max(ADT.event_time_jittered) AS time_out\n",
    "FROM `starr_datalake2018.adt` AS ADT\n",
    "INNER JOIN `noshad_test.cohort_AL_user_role` AS CH\n",
    "  USING (pat_enc_csn_id_coded)\n",
    "GROUP BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, ADT.department_id, CH.tpaAdminTime\n",
    "ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(query);\n",
    "\n",
    "#results = cursor.fetchall();\n",
    "#print(results[:2])\n",
    "#results_np = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main part to generate the transaction rate associated with other patients per each cohort_patient\n",
    "# CREATE OR REPLACE TABLE noshad.num_tnx\n",
    "\n",
    "\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();\n",
    "\n",
    "# Upload time_mapping tmp.CSV \n",
    "\n",
    "schemafield_col1 = bigquery.schema.SchemaField(\"ANON_ID\",\"STRING\") #Define your schema\n",
    "schemafield_col2 = bigquery.schema.SchemaField(\"JITTER\",\"INTEGER\")\n",
    "\n",
    "filename = 'tmp.csv'\n",
    "table_id = 'tmp' # the name of the chart to create\n",
    "\n",
    "dataset_ref = client.dataset('noshad')\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "\n",
    "with open(filename, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print('temporary tmp file generated')\n",
    "\n",
    "# Main part to generate num_pat per each patient\n",
    "query= \"\"\"\n",
    "\n",
    "CREATE OR REPLACE TABLE noshad.num_tnx AS(\n",
    "WITH\n",
    "\n",
    "    -- Generate ADT_cohort with actual times\n",
    "  ADT_real_date AS\n",
    "  (SELECT ADT.* except(time_in,time_out,tpaAdminTime), DATETIME_SUB(ADT.time_in, INTERVAL TMP.JITTER DAY) as time_in,  \n",
    "  DATETIME_SUB(ADT.time_out, INTERVAL TMP.JITTER DAY) as time_out, \n",
    "  DATETIME_SUB(ADT.tpaAdminTime, INTERVAL TMP.JITTER DAY) as tpaAdminTime\n",
    "  \n",
    "  FROM `noshad.ADT_cohort_jit` as ADT,\n",
    "  `noshad.tmp` as TMP\n",
    "  \n",
    "  WHERE ADT.jc_uid=TMP.ANON_ID\n",
    "  \n",
    "  ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    "  ),\n",
    "  \n",
    "    -- Generate AL with actual times\n",
    "  AL_real_date AS\n",
    "  (SELECT AL.*, DATETIME_SUB(AL.access_time_jittered, INTERVAL TMP.JITTER DAY) as access_time_real\n",
    "  FROM `noshad.shc_access_log_de_dep_id` as AL,\n",
    "  `noshad.tmp` as TMP\n",
    "  WHERE AL.rit_uid=TMP.ANON_ID\n",
    "  ORDER BY AL.rit_uid\n",
    "  ),\n",
    "  \n",
    "  --- Generate NUM of PAT PER DEP with times\n",
    "  NUM_PAT_PER_DEP AS \n",
    "  (SELECT ADT_real_date.*, count(*) as num_tranx , \n",
    "    count(*)/ DATETIME_DIFF(ADT_real_date.time_out, ADT_real_date.time_in, MINUTE) as num_tranx_rate \n",
    "  FROM ADT_real_date, AL_real_date\n",
    "  WHERE \n",
    "    AL_real_date.department_id = ADT_real_date.department_id\n",
    "    AND ADT_real_date.time_in < AL_real_date.access_time_real \n",
    "    AND AL_real_date.access_time_real < ADT_real_date.time_out\n",
    "    \n",
    "  GROUP BY ADT_real_date.jc_uid, ADT_real_date.pat_enc_csn_id_coded, \n",
    "    ADT_real_date.department_id, ADT_real_date.tpaAdminTime, \n",
    "    ADT_real_date.time_in, ADT_real_date.time_out\n",
    "  ORDER BY ADT_real_date.department_id)\n",
    "\n",
    "  -- Main script\n",
    "SELECT jc_uid, pat_enc_csn_id_coded, max(num_tranx_rate) as max_norm_num_tranx\n",
    "FROM NUM_PAT_PER_DEP\n",
    "WHERE time_in < tpaAdminTime\n",
    "GROUP BY jc_uid, pat_enc_csn_id_coded\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query);\n",
    "\n",
    "print('feature extraced')\n",
    "\n",
    "## Final step: delete the temporary time mapping\n",
    "query = \"DROP TABLE noshad.tmp\"\n",
    "cursor.execute(query);\n",
    "\n",
    "print('temporary tmp file deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporary tmp file generated\n",
      "feature extraced\n",
      "temporary tmp file deleted\n"
     ]
    }
   ],
   "source": [
    "## Number of unique patients per each cohort_patient in ED\n",
    "# CREATE OR REPLACE TABLE noshad.num_unq_pat\n",
    "\n",
    "\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();\n",
    "\n",
    "# Upload time_mapping tmp.CSV \n",
    "\n",
    "schemafield_col1 = bigquery.schema.SchemaField(\"ANON_ID\",\"STRING\") #Define your schema\n",
    "schemafield_col2 = bigquery.schema.SchemaField(\"JITTER\",\"INTEGER\")\n",
    "\n",
    "filename = 'tmp.csv'\n",
    "table_id = 'tmp' # the name of the chart to create\n",
    "\n",
    "dataset_ref = client.dataset('noshad')\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "\n",
    "with open(filename, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print('temporary tmp file generated')\n",
    "\n",
    "# Main part to generate num_pat per each patient\n",
    "query= \"\"\"\n",
    "\n",
    "CREATE OR REPLACE TABLE noshad.num_unq_pat AS(\n",
    "WITH\n",
    "\n",
    "    -- Generate ADT_cohort with actual times\n",
    "  ADT_real_date AS\n",
    "  (SELECT ADT.* except(time_in,time_out,tpaAdminTime), DATETIME_SUB(ADT.time_in, INTERVAL TMP.JITTER DAY) as time_in,  \n",
    "  DATETIME_SUB(ADT.time_out, INTERVAL TMP.JITTER DAY) as time_out, \n",
    "  DATETIME_SUB(ADT.tpaAdminTime, INTERVAL TMP.JITTER DAY) as tpaAdminTime\n",
    "  \n",
    "  FROM `noshad.ADT_cohort_jit` as ADT,\n",
    "  `noshad.tmp` as TMP\n",
    "  \n",
    "  WHERE ADT.jc_uid=TMP.ANON_ID\n",
    "  \n",
    "  ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    "  ),\n",
    "  \n",
    "    -- Generate AL with actual times\n",
    "  AL_real_date AS\n",
    "  (SELECT AL.*, DATETIME_SUB(AL.access_time_jittered, INTERVAL TMP.JITTER DAY) as access_time_real\n",
    "  FROM `noshad.shc_access_log_de_dep_id` as AL,\n",
    "  `noshad.tmp` as TMP\n",
    "  WHERE AL.rit_uid=TMP.ANON_ID\n",
    "  ORDER BY AL.rit_uid\n",
    "  ),\n",
    "  \n",
    "  --- Generate NUM of Unique PAT PER DEP with times\n",
    "  NUM_PAT_PER_DEP AS \n",
    "  (SELECT ADT_real_date.*, count(DISTINCT AL_real_date.rit_uid) as num_unq_pat , \n",
    "    count(DISTINCT AL_real_date.rit_uid)/ DATETIME_DIFF(ADT_real_date.time_out, ADT_real_date.time_in, MINUTE) as num_unq_pat_rate \n",
    "  FROM ADT_real_date, AL_real_date\n",
    "  WHERE \n",
    "    AL_real_date.department_id = ADT_real_date.department_id\n",
    "    AND ADT_real_date.time_in < AL_real_date.access_time_real \n",
    "    AND AL_real_date.access_time_real < ADT_real_date.time_out\n",
    "    \n",
    "  GROUP BY ADT_real_date.jc_uid, ADT_real_date.pat_enc_csn_id_coded, \n",
    "    ADT_real_date.department_id, ADT_real_date.tpaAdminTime, \n",
    "    ADT_real_date.time_in, ADT_real_date.time_out\n",
    "  ORDER BY ADT_real_date.department_id)\n",
    "\n",
    "  -- Main script\n",
    "SELECT jc_uid, pat_enc_csn_id_coded, min(num_unq_pat) as max_num_unq_pat\n",
    "FROM NUM_PAT_PER_DEP\n",
    "WHERE time_in < tpaAdminTime\n",
    "GROUP BY jc_uid, pat_enc_csn_id_coded\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query);\n",
    "\n",
    "print('feature extraced')\n",
    "\n",
    "## Final step: delete the temporary time mapping\n",
    "query = \"DROP TABLE noshad.tmp\"\n",
    "cursor.execute(query);\n",
    "\n",
    "print('temporary tmp file deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporary tmp file generated\n",
      "feature extraced\n",
      "temporary tmp file deleted\n"
     ]
    }
   ],
   "source": [
    "## main part to generate the number of new incoming patients (admitted or transferred in) per each cohort_patient\n",
    "# CREATE OR REPLACE TABLE noshad.num_new_pat\n",
    "\n",
    "\n",
    "client = bigquery.Client(\"som-nero-phi-jonc101\"); # Project identifier\n",
    "conn = dbapi.connect(client);\n",
    "cursor = conn.cursor();\n",
    "\n",
    "# Upload time_mapping tmp.CSV \n",
    "\n",
    "schemafield_col1 = bigquery.schema.SchemaField(\"ANON_ID\",\"STRING\") #Define your schema\n",
    "schemafield_col2 = bigquery.schema.SchemaField(\"JITTER\",\"INTEGER\")\n",
    "\n",
    "filename = 'tmp.csv'\n",
    "table_id = 'tmp' # the name of the chart to create\n",
    "\n",
    "dataset_ref = client.dataset('noshad')\n",
    "table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "\n",
    "with open(filename, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print('temporary tmp file generated')\n",
    "\n",
    "# Main part to generate num_pat per each patient\n",
    "query= \"\"\"\n",
    "\n",
    "CREATE OR REPLACE TABLE noshad.num_new_pat AS(\n",
    "WITH\n",
    "\n",
    "    -- Generate ADT_cohort with actual times\n",
    "  ADT_real_date AS\n",
    "  (SELECT ADT.* except(time_in,time_out,tpaAdminTime), DATETIME_SUB(ADT.time_in, INTERVAL TMP.JITTER DAY) as time_in,  \n",
    "  DATETIME_SUB(ADT.time_out, INTERVAL TMP.JITTER DAY) as time_out, \n",
    "  DATETIME_SUB(ADT.tpaAdminTime, INTERVAL TMP.JITTER DAY) as tpaAdminTime\n",
    "  \n",
    "  FROM `noshad.ADT_cohort_jit` as ADT,\n",
    "  `noshad.tmp` as TMP\n",
    "  \n",
    "  WHERE ADT.jc_uid=TMP.ANON_ID\n",
    "  \n",
    "  ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    "  ),\n",
    "  \n",
    "  \n",
    "      -- Generate ADT_summary (pertaining to all patients) with actual times\n",
    "  ADT_all_real_date AS\n",
    "  (SELECT ADT.* except(in_time,out_time), DATETIME_SUB(ADT.in_time, INTERVAL TMP.JITTER DAY) as time_in,  \n",
    "  DATETIME_SUB(ADT.out_time, INTERVAL TMP.JITTER DAY) as time_out\n",
    "  \n",
    "  FROM `noshad.adt_summary` as ADT,\n",
    "  `noshad.tmp` as TMP\n",
    "  \n",
    "  WHERE ADT.jc_uid=TMP.ANON_ID\n",
    "  \n",
    "  ORDER BY ADT.jc_uid, ADT.pat_enc_csn_id_coded, time_in\n",
    "  ),\n",
    "  \n",
    "  \n",
    "  --- Generate NUM of New PAT PER DEP with times\n",
    "  NUM_PAT_PER_DEP AS \n",
    "  (SELECT ADT_real_date.*, count(*) as num_new_pat , \n",
    "    count(*)/ DATETIME_DIFF(ADT_real_date.time_out, ADT_real_date.time_in, MINUTE) as num_new_pat_rate \n",
    "  FROM ADT_real_date, ADT_all_real_date\n",
    "  WHERE \n",
    "    ADT_all_real_date.department_id = ADT_real_date.department_id\n",
    "    AND ADT_real_date.time_in < ADT_all_real_date.time_in\n",
    "    AND ADT_all_real_date.time_in < ADT_real_date.time_out\n",
    "    \n",
    "  GROUP BY ADT_real_date.jc_uid, ADT_real_date.pat_enc_csn_id_coded, \n",
    "    ADT_real_date.department_id, ADT_real_date.tpaAdminTime, \n",
    "    ADT_real_date.time_in, ADT_real_date.time_out\n",
    "  ORDER BY ADT_real_date.department_id)\n",
    "\n",
    "  -- Main script\n",
    "SELECT jc_uid, pat_enc_csn_id_coded, max(num_new_pat_rate) as max_num_new_pat_rate,  sum(num_new_pat_rate) as sum_num_new_pat_rate\n",
    "FROM NUM_PAT_PER_DEP\n",
    "WHERE time_in < tpaAdminTime\n",
    "GROUP BY jc_uid, pat_enc_csn_id_coded\n",
    ")\"\"\"\n",
    "\n",
    "cursor.execute(query);\n",
    "\n",
    "print('feature extraced')\n",
    "\n",
    "## Final step: delete the temporary time mapping\n",
    "query = \"DROP TABLE noshad.tmp\"\n",
    "cursor.execute(query);\n",
    "\n",
    "print('temporary tmp file deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
